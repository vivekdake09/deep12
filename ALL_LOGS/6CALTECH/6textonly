6. Object detection using Transfer Learning of CNN architectures
a. Load in a pre-trained CNN model trained on a large dataset
b. Freeze parameters (weights) in model’s lower convolutional layers
c. Add custom classifier with several layers of trainable parameters to model
d. Train classifier layers on training data available for task
e. Fine-tune hyper parameters and unfreeze more layers as needed

dataset: Caltech






import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np








dataset_dir = "caltech-101-img/"
dataset_datagen = ImageDataGenerator(
    rescale=1.0 / 255,
)

# here batch_size is the number of images in each batch
batch_size = 2000
dataset_generator = dataset_datagen.flow_from_directory(
    dataset_dir,
    target_size=(64, 64),
    batch_size=batch_size,
    class_mode='categorical'
)









x_train, y_train =  dataset_generator[0]
x_test, y_test = dataset_generator[1]

print(len(x_train))
print(len(x_test))








# Load VGG16 without top layers
weights_path = "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))







for layer in base_model.layers:
   layer.trainable = False







x = Flatten()(base_model.output)
x = Dense(64, activation='relu')(x)
predictions = Dense(102, activation='softmax')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)
# Compile the model
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])







# Train the model
model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))








import matplotlib.pyplot as plt
predicted_value = model.predict(x_test)








labels = list(dataset_generator.class_indices.keys())







n = 1000
plt.imshow(x_test[n])
print("Preditcted: ",labels[np.argmax(predicted_value[n])])
print("Actual: ", labels[np.argmax(y_test[n])])


“In this practical, I implemented object classification using transfer learning on the Caltech-101 dataset. I used VGG16 as a pretrained CNN model trained on ImageNet. First, I loaded and normalized images using ImageDataGenerator, and resized them to 64×64. I loaded the VGG16 model without the top classifier layers, keeping only the convolutional backbone. These layers were frozen so that their pretrained weights remain unchanged.”

“Next, I added my own custom classifier: a Flatten layer, a Dense layer with 64 neurons, and a final Dense layer with 102 softmax outputs for the Caltech-101 classes. I trained only the new classifier layers while keeping the base VGG16 frozen. After training, I evaluated the model on test images and visualized predictions. This practical demonstrates how to reuse a powerful CNN model for a new image-classification task with much less data and training time.”

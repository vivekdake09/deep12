
‚ÄúIn this practical, I implemented anomaly detection using an autoencoder on the credit-card fraud dataset. First, I loaded the dataset and normalized all features using StandardScaler so the network trains properly. Since autoencoders learn how to reconstruct normal patterns, I used the transaction data (excluding labels) both as input and output for training.‚Äù

‚ÄúThe encoder compresses the transaction into a 16-dimensional latent vector, and the decoder reconstructs it back to the original 30 features. I trained the autoencoder using the Adam optimizer and mean squared error loss because reconstruction accuracy is important.‚Äù

‚ÄúAfter training, I passed test data through the model and calculated reconstruction error for each transaction. Normal transactions are reconstructed well, whereas fraudulent ones produce high MSE. Using a threshold, I classified high-error samples as anomalies. Finally, I evaluated performance using confusion matri

NumPy ‚Üí numerical operations

Pandas ‚Üí load creditcard.csv

TensorFlow/Keras ‚Üí build autoencoder

StandardScaler ‚Üí normalize dataset

train_test_split ‚Üí divide into train/test

confusion_matrix, classification_report ‚Üí evaluate anomaly detection

Matplotlib ‚Üí plot reconstruction error distribution

layers, models ‚Üí build encoder and decode

Separate features and labels
X = scaler.fit_transform(dataset.drop("Class", axis=1))
y = dataset["Class"]


‚úî Normal transactions = 0
‚úî Fraud transactions = 1
‚úî StandardScaler normalizes features so neural network trains properly.

Split into train/test


‚úÖ C. Build Encoder (Compress High-Dim Data)
‚úî Encoder compresses high-dim credit-card transaction features
‚úî Learns compressed representation (‚Äúlatent vector‚Äù)
‚úî ReLU adds non-linearity


‚úÖ D. Build Decoder (Reconstruct Original Input)

‚úî Decoder expands latent vector back to original features
‚úî Output activation = linear because input values are continuous



‚úÖ Autoencoder = Encoder + Decoder

‚úî Loss = MSE because we want accurate reconstruction
‚úî Optimizer = Adam ‚Üí fast, adaptive learning


‚úÖ Train Autoencoder
‚úî Autoencoder tries to reconstruct normal transactions
‚úî Fraudulent transactions ‚Üí poor reconstruction ‚Üí higher error (MSE)
‚úî Input = Output = X (unsupervised learning)


‚úÖ Compute Reconstruction Error
‚úî MSE measures how well each sample is reconstructed
‚úî High MSE indicates anomaly/fraud


‚úî Threshold decides if error is high enough to mark anomaly
‚úî If MSE > threshold ‚Üí flagged as fraud



1Ô∏è‚É£ What is an autoencoder?

A neural network that learns to reconstruct its input using an encoder and decoder.

2Ô∏è‚É£ Why use autoencoder for anomaly detection?

It learns normal patterns; anomalies reconstruct poorly and give high error.

3Ô∏è‚É£ What is latent representation?

A compressed low-dimensional encoding of the input features.

4Ô∏è‚É£ Why is MSE used as loss?

It measures how different reconstruction is from the original.

5Ô∏è‚É£ Why use StandardScaler?

To normalize features for stable and faster training.

‚úÖ Practical Questions
6Ô∏è‚É£ Why input and output both are X?

Autoencoder is unsupervised and attempts to reconstruct the same input.

7Ô∏è‚É£ Why linear activation in final decoder layer?

To output continuous numeric values matching original data.

8Ô∏è‚É£ What happens if threshold is too low?

Many normal samples are falsely marked as anomalies.

9Ô∏è‚É£ What if latent dimension is too small?

Reconstruction becomes bad ‚Üí more false positives.

üîü Why choose Adam optimizer?

It converges faster and handles sparse, noisy data well.

11Ô∏è‚É£ Why batch_size = 32?

Good balance between memory usage and gradient stability.

12Ô∏è‚É£ Why split into train/test?

Train on normal patterns, test on real data to detect anomalies.

‚úÖ Advanced Viva Questions
13Ô∏è‚É£ How does backpropagation work here?

It updates encoder and decoder weights to minimize reconstruction error.

14Ô∏è‚É£ Why is anomaly detection unsupervised?

Because labels may not be available or fraud cases are very rare.

15Ô∏è‚É£ What are false positives and false negatives?

FP = normal marked as fraud; FN = fraud missed ‚Äî critical in banking.



“In this practical, I implemented object classification using transfer learning. I used the CIFAR-10 dataset and a pretrained VGG16 model, which was originally trained on ImageNet. First, I loaded and normalized the CIFAR images using ImageDataGenerator. Then I loaded VGG16 with include_top=False to remove its classifier, keeping only the convolutional feature extractor. I froze its lower layers to preserve pretrained weights and added my own custom classifier consisting of Flatten, Dense(256), and Dense(10) with Softmax.”

“I trained only the custom classifier first, treating VGG16 as a fixed feature extractor. After initial training, I fine-tuned the model by unfreezing the last four layers of VGG16 with a small learning rate so the model could adapt better to CIFAR-10 features. Finally, I evaluated the model on test images and displayed predictions along with actual labels. This demonstrates how transfer learning reduces training time and improves accuracy by reusing knowledge from pretrained CNNs.”


What each does:

VGG16 → Pretrained CNN model (on ImageNet)

Model → Functional API for modifying pretrained models

Dense, Flatten → Custom classifier layers

ImageDataGenerator → Load CIFAR-10 images from folders

Adam → Optimizer for training

✅ B. Load and Preprocess CIFAR-10 Dataset
Create data generators
train_datagen = ImageDataGenerator(rescale=1.0/255)
test_datagen = ImageDataGenerator(rescale=1.0/255)


✔ Normalizes pixel values
✔ Essential for CNN training


✅ C. Load Pretrained VGG16 (Transfer Learning)
base_model = VGG16(weights=weights_path, include_top=False, input_shape=(32,32,3))


✔ Loads VGG16 WITHOUT classifier head
✔ Uses pretrained convolution layers
✔ Pretrained on ImageNet → millions of images
✔ Keeps its knowledge about edges, textures, shapes

✅ D. Freeze Pretrained Layers
for layer in base_model.layers:
    layer.trainable = False


✔ Freezes lower convolution layers
✔ Prevents updating pretrained weights
✔ We only train our custom classifier on top

✅ E. Add Custom Classifier
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)


✔ Flatten → convert feature maps into 1D vector
✔ Dense(256) → learn patterns specific to CIFAR-10
✔ Dense(10, softmax) → 10 output classes (CIFAR-10 dataset)

Create & Compile final model
model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])


✔ Adam → adaptive, fast
✔ Loss → multi-class classification
✔ Tracks accuracy

✅ F. Train Custom Classifier
model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))


✔ Only classifier layers train
✔ Pretrained layers remain unchanged

✅ G. Fine-Tuning (Advanced Transfer Learning)
Reload base model
base_model = VGG16(...include_top=False...)

Freeze all layers first
for layer in base_model.layers:
    layer.trainable = False

Unfreeze last 4 layers only
for layer in base_model.layers[-4:]:
    layer.trainable = True


✔ Allows deeper layers to adjust to CIFAR-10 dataset
✔ Fine-tuning improves accuracy

Rebuild classifier and train again
model.compile(optimizer=Adam(learning_rate=0.001), ...)
model.fit(...)


✔ Smaller learning rate for fine-tuning

✅ H. Prediction
predicted_value = model.predict(x_test)
labels = list(test_generator.class_indices.keys())
plt.imshow(x_test[n])
print("Predicted: ", labels[argmax])


✔ Shows actual vs predicted class
✔ Visual verification of model performance


✅ Theory Questions
1. What is transfer learning?

Using a pretrained CNN model trained on a large dataset and adapting it to a new task.

2. Why freeze layers?

To preserve pretrained knowledge and avoid retraining huge networks.

3. Why unfreeze last layers during fine-tuning?

Because deeper layers learn high-level features more relevant to the new dataset.

4. What is VGG16?

A pretrained CNN architecture with 16 layers trained on ImageNet.

5. Why use softmax in output?

Softmax gives probability distribution across 10 classes.

✅ Practical Questions
6. Why target_size=32x32?

CIFAR-10 images are originally 32×32.

7. Why categorical class mode?

Because CIFAR-10 has 10 classes (multi-class classification).

8. Why include_top=False?

To remove ImageNet classifier and add custom classifier.

9. Why use Adam optimizer?

Adaptive learning rate → trains faster.

10. Why convert the feature maps to 1D?

Dense classifier only accepts 1D inputs → done using Flatten.

11. Why load full dataset with a large batch?

Because CIFAR-10 is small and fits in memory.

✅ Advanced Questions
12. What happens if you do not freeze layers?

The model may overfit or destroy pretrained knowledge.

13. Difference between feature extraction and fine-tuning?

Feature extraction = freeze all layers
Fine-tuning = unfreeze some deeper layers

14. Why learning rate = 0.001 in fine-tuning?

Small LR prevents damaging pretrained weights.

15. Why VGG16 is good for transfer learning?

It has strong general-purpose filters trained on millions of images.

✅ PART 4 — 1–2 MINUTE SUMMARY FOR EXTERNAL

